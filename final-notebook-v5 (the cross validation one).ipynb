{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the other object as test set ðŸ¤ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus cell for using the different test set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from copy import copy\n",
    "from matplotlib import cm, colors\n",
    "import cv2\n",
    "\n",
    "# TvÃ¥ delar:\n",
    "# 1: fixa segmentstorleken\n",
    "# 2: fixa masker fÃ¶r varje segment\n",
    "# Eller egentligen, skit i det. Applicera masken i fÃ¶rvÃ¤g?\n",
    "\n",
    "\n",
    "def getTestObjects(porositythreshold=0.5): \n",
    "    objectwidth = 100\n",
    "    objectheight = 100\n",
    "    xspacing = 116\n",
    "    yspacing = 300\n",
    "    xstart = 293\n",
    "    ystart = 445\n",
    "    xend = 1730\n",
    "    yend = 1770\n",
    "    powderthickness = 80\n",
    "    endlayer = 225\n",
    "    vsegments = [0, 150, 187, endlayer]\n",
    "\n",
    "    paths = pathlib.Path('./OT data 80 um/int').glob('*.tif')\n",
    "    paths_sorted = [x for x in paths]\n",
    "    paths_sorted.sort()\n",
    "    integrals = np.array([np.array(plt.imread(path)) for path in paths_sorted])\n",
    "\n",
    "    objectinfo = pd.read_csv('Parameters2.csv', names=[\"Object\", \"P\", \"S\", \"H\", \"Porosity\", \"Label\"])\n",
    "    objectCoordinates = [[x, x+objectwidth, y, y+objectheight] for y in reversed(range(\n",
    "        ystart, yend, objectheight + yspacing)) for x in range(xstart, xend, xspacing + objectwidth)]\n",
    "    coorddf = pd.DataFrame(objectCoordinates, columns=['xstart', 'xend', 'ystart', 'yend'])\n",
    "    objectinfo = coorddf.join(objectinfo)\n",
    "\n",
    "    objectinfo.replace('GOOD', 0, inplace=True)\n",
    "    objectinfo.replace('LOF', 1, inplace=True)\n",
    "    objectinfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    objects = np.full((len(objectinfo), endlayer, objectheight, objectwidth), np.nan)\n",
    "\n",
    "    for index, object in objectinfo.iterrows():\n",
    "        objects[index] = integrals[:, object.ystart:object.yend, object.xstart:object.xend]\n",
    "\n",
    "    rtn = np.full(objects.shape, np.nan)\n",
    "    aggregate = np.sum(objects, axis=(0))\n",
    "\n",
    "    emptyRatio = 30\n",
    "    limit = np.percentile(aggregate, emptyRatio)\n",
    "    testmask = aggregate >= limit\n",
    "    for object in objects:\n",
    "        object[~testmask] = np.nan\n",
    "\n",
    "    # Create the good frame\n",
    "    segmentdf = pd.read_csv('Segments2.csv', names=[\"Object\", \"Objectnumber\", \"Segment\", \"P\", \"S\", \"H\", \"Porosity\"])\n",
    "    segmentdf.insert(1, \"VED\", segmentdf.P * 1000/(segmentdf.S * segmentdf.H * powderthickness))\n",
    "    segmentdf.insert(1, \"Label\", np.where(segmentdf.Porosity > porositythreshold, 1, 0))\n",
    "    # segmentdf.drop(segmentdf[(segmentdf.VED > 50)].index, inplace=True)\n",
    "    segmentdf.reset_index(drop=True, inplace=True)\n",
    "    vs = [[vsegments[j], vsegments[j+1]] for i in range(0, len(objects)) for j in reversed(range(0, len(vsegments)-1))]\n",
    "    coorddf = pd.DataFrame(vs, columns=['zstart', 'zend'])\n",
    "    testobjectinfo = coorddf.join(segmentdf)\n",
    "    testobjectinfo.drop(testobjectinfo[(testobjectinfo.Objectnumber == 28) | (testobjectinfo.Objectnumber == 21) ].index, inplace=True)\n",
    "    testobjectinfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    testobjects = [objects[object.Objectnumber-1, object.zstart:object.zend] for index, object in testobjectinfo.iterrows()]\n",
    "\n",
    "    del objects\n",
    "    del objectinfo\n",
    "    del coorddf\n",
    "    del objectCoordinates\n",
    "    del paths_sorted\n",
    "    del integrals\n",
    "    print(\"fetching data with \", porositythreshold)\n",
    "    return testobjects, testobjectinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from copy import copy\n",
    "from matplotlib import cm, colors\n",
    "import cv2\n",
    "\n",
    "def getTrainObjects(objectsplit=1, upsamplingratio=1, positive_multiplier=1, porositythreshold=0.5, separate_test=True):\n",
    "    emptyRatio = 47\n",
    "    objectwidth = 83\n",
    "    objectheight = 122\n",
    "    xspacing = 133\n",
    "    yspacing = 270\n",
    "    xstart = 293\n",
    "    ystart = 268\n",
    "    xend = 1730\n",
    "    yend = 1770\n",
    "    hsegments = [0,26,50,74,98,122]\n",
    "    powderthickness = 80\n",
    "    endlayer = 187\n",
    "\n",
    "    paths = pathlib.Path('./OT data 80 um/int').glob('*.tif')\n",
    "    paths_sorted = [x for x in paths]\n",
    "    paths_sorted.sort()\n",
    "    block = np.array([np.array(plt.imread(path)) for path in paths_sorted])\n",
    "    integrals = block[0:endlayer]\n",
    "\n",
    "    del paths_sorted\n",
    "    objectinfo = pd.read_csv('Parameters.csv', names=[\"Object\", \"P\", \"S\", \"H\", \"Porosity\", \"Label\"])\n",
    "\n",
    "    objectCoordinates = [[x, x+objectwidth, y, y+objectheight] for y in reversed(range(\n",
    "        ystart, yend, objectheight + yspacing)) for x in range(xstart, xend, xspacing + objectwidth)]\n",
    "    coorddf = pd.DataFrame(objectCoordinates, columns=['xstart', 'xend', 'ystart', 'yend'])\n",
    "    objectinfo = coorddf.join(objectinfo)\n",
    "\n",
    "    objects = np.full((len(objectinfo), endlayer, objectheight, objectwidth), np.nan)\n",
    "\n",
    "    for index, object in objectinfo.iterrows():\n",
    "        objects[index] = integrals[:, object.ystart:object.yend, object.xstart:object.xend]\n",
    "\n",
    "    aggregate = np.sum(objects, axis=(0,1))\n",
    "\n",
    "    emptyRatio = 47\n",
    "    limit = np.percentile(aggregate, emptyRatio)\n",
    "    mask = aggregate >= limit\n",
    "    mask = np.repeat([mask], endlayer, 0)\n",
    "\n",
    "    for object in objects:\n",
    "        object[~mask] = np.nan\n",
    "\n",
    "    # Time to construct the \"real\" dataframe\n",
    "\n",
    "    segmentdf = pd.read_csv('Segments.csv', names=[\"Object\", \"Objectnumber\", \"Segment\", \"P\", \"S\", \"H\", \"Porosity\", \"Area\"])\n",
    "    segmentdf.insert(1, \"VED\", segmentdf.P * 1000/(segmentdf.S * segmentdf.H * powderthickness))\n",
    "    segmentdf.insert(1, \"Label\", np.where(segmentdf.Porosity > porositythreshold, 1, 0))\n",
    "    originalframe = segmentdf.copy()\n",
    "    hs = [[hsegments[j], hsegments[j+1]] for i in range(0, len(objects)) for j in range(0, len(hsegments)-1)]\n",
    "    coorddf = pd.DataFrame(hs, columns=['hstart', 'hend'])\n",
    "    segmentdf = coorddf.join(segmentdf)\n",
    "\n",
    "    # Start of object multiplication \n",
    "    layersPerObject = endlayer // objectsplit\n",
    "    testEnd = endlayer if separate_test else endlayer - layersPerObject * (objectsplit // 3)\n",
    "    zs = [segmentdf.copy().assign(zstart=z, zend=z+layersPerObject) for z in range(0, testEnd-layersPerObject+1, layersPerObject//(upsamplingratio * positive_multiplier))]\n",
    "    testzs = [segmentdf.copy().assign(zstart=testEnd, zend=endlayer)]\n",
    "    trainobjectinfo = pd.concat(zs, ignore_index=True)\n",
    "    trainobjectinfo.drop(trainobjectinfo[(trainobjectinfo.Objectnumber == 28) | (trainobjectinfo.Objectnumber == 21) ].index, inplace=True)\n",
    "    trainobjectinfo.reset_index(drop=True, inplace=True)\n",
    "    testobjectinfo = pd.concat(testzs, ignore_index=True)\n",
    "    testobjectinfo.drop(testobjectinfo[(testobjectinfo.Objectnumber == 28) | (testobjectinfo.Objectnumber == 21) ].index, inplace=True)\n",
    "    testobjectinfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Removes extra rows \n",
    "    trainobjectinfo = trainobjectinfo[(trainobjectinfo['Label'] == 1) | (trainobjectinfo['zstart'] % (positive_multiplier) == 0)]\n",
    "\n",
    "    trainobjects = [objects[object.Objectnumber-1, object.zstart:object.zend, object.hstart:object.hend] for index, object in trainobjectinfo.iterrows()]\n",
    "    testobjects = [objects[object.Objectnumber-1, object.zstart:object.zend, object.hstart:object.hend] for index, object in testobjectinfo.iterrows()]\n",
    "\n",
    "    print(\"fetching data with objectsplit: {}, upsamplingratio: {}, positive_multiplier: {}, porositythreshold: {}\".format(objectsplit, upsamplingratio, positive_multiplier, porositythreshold))\n",
    "    return trainobjects, trainobjectinfo, testobjects, testobjectinfo\n",
    "\n",
    "# assert(np.average(np.isfinite(trainobjects)) == 1)\n",
    "# assert(np.average(np.isfinite(testobjects)) == 1)\n",
    "# xs = np.copy(aggregate)\n",
    "# xs[~backgroundmask] = np.nan\n",
    "# plt.imshow(xs)\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, metrics\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import tree\n",
    "import warnings\n",
    "\n",
    "def preprocess(objects, type, sharpening):\n",
    "    rtn = []\n",
    "    # print(rtn.shape)\n",
    "    for index, object in enumerate(objects):\n",
    "        object = np.copy(object)\n",
    "        sharpened = object\n",
    "        if(sharpening != 'none'):\n",
    "            sharpeningKernel = np.array([   [-1, -1,  -1],\n",
    "                                        [-1,  9,  -1],\n",
    "                                        [ -1, -1,  -1]\n",
    "        ]) if sharpening == 'diagonal' else np.array([  [0, -1,  0],\n",
    "                                                        [-1, 5, -1],\n",
    "                                                        [0, -1,  0]])\n",
    "            sharpened = np.array([cv2.filter2D(src=image, ddepth=-1, kernel=sharpeningKernel) for image in object])\n",
    "        # Sharpening is done\n",
    "        if type == 'scatter' or type == 'spatstat':\n",
    "            xs = np.array(sharpened, copy=True, dtype=np.float32)\n",
    "            rtn.append(xs)\n",
    "        elif type == 'moran':\n",
    "            xs = np.array(sharpened, copy=True, dtype=np.float32)\n",
    "            avg = np.nanmean(xs)\n",
    "            stddev = np.nanstd(xs)\n",
    "            xs = (xs - avg) / avg\n",
    "            rtn.append(xs)\n",
    "    return rtn\n",
    "\n",
    "\n",
    "def calculateoutliers(objects, type, neighbourhoodSetting, windowSize):\n",
    "    # c, z, y, x = objects.shape\n",
    "\n",
    "\n",
    "    outlierValues = []\n",
    "    index = 0\n",
    "    # return objects\n",
    "    for object in objects:\n",
    "        object = np.copy(object)\n",
    "        z, y, x = object.shape\n",
    "        # Step 1: calculate neighbourhood\n",
    "        neighbourkernel = np.ones((neighbourhoodSetting, neighbourhoodSetting)) / neighbourhoodSetting**2\n",
    "        flatNeighbourhood = np.array([cv2.filter2D(src=layer, ddepth=-1, kernel=neighbourkernel) for layer in object])\n",
    "        neighbourhoodValues = np.array([\n",
    "            np.sum(flatNeighbourhood[layerIndex-windowSize:layerIndex], axis=0)/windowSize\n",
    "            for layerIndex in range(windowSize, z+1)\n",
    "        ])\n",
    "        # Step 2: calculate outlier\n",
    "        offset = windowSize // 2\n",
    "        endoffset = windowSize - offset - 1\n",
    "\n",
    "        ys = neighbourhoodValues[0:z-windowSize+1]\n",
    "        xs = object[offset:z-endoffset]\n",
    "        filter = np.logical_and(np.isfinite(xs), np.isfinite(ys))\n",
    "\n",
    "        # plt.imshow(xs[0])\n",
    "        # plt.figure()\n",
    "        # plt.imshow(xs[0])\n",
    "        # plt.figure()\n",
    "        # if(index == 58):\n",
    "        #     plt.imshow(xs[0])\n",
    "        #     plt.figure()\n",
    "        #     plt.imshow(ys[0])\n",
    "        #     plt.figure()\n",
    "        #     plt.imshow(filter[0])\n",
    "        #     plt.figure()\n",
    "        #     print(len(np.unique(filter)))\n",
    "        numberOfFilterValues = len(np.unique(filter))\n",
    "        # print(\"filterlength is: \", numberOfFilterValues)\n",
    "        # print(\"index is:\", index)\n",
    "        assert numberOfFilterValues == 2, f\"Expected filter to have two values, got: {numberOfFilterValues}\"\n",
    "        if type == 'spatstat':\n",
    "            outliers = xs - ys\n",
    "            avg = np.mean(outliers[filter])\n",
    "            std = np.std(outliers[filter])\n",
    "            outliers = (outliers - avg) / std\n",
    "            outlierValues.append(outliers)\n",
    "        else:\n",
    "            with warnings.catch_warnings():\n",
    "                line = np.polyfit(xs[filter].flatten(), ys[filter].flatten(), 1)\n",
    "                p = np.poly1d(line)\n",
    "                outlierValues.append(p(xs) - ys)\n",
    "            assert(xs.shape == p(ys).shape)\n",
    "        assert(len(np.unique(outlierValues[index])) > 1)\n",
    "        assert(len(np.unique(np.isfinite(outlierValues[index]))) == 2)\n",
    "        index+=1\n",
    "    return outlierValues\n",
    "\n",
    "def encode(outlierobjects, buckets, minval=0, maxval=0):\n",
    "    numberOfObjects = len(outlierobjects)\n",
    "    X = np.full((numberOfObjects, buckets), np.nan)\n",
    "    raw = np.concatenate([oo.flatten() for oo in outlierobjects])\n",
    "    filter = np.isfinite(raw)\n",
    "    minval = np.min(raw[filter]) if minval == 0 else minval\n",
    "    maxval = np.max(raw[filter]) if maxval == 0 else maxval\n",
    "    for index in range(0, numberOfObjects):\n",
    "        xs = outlierobjects[index]\n",
    "        filter = np.isfinite(xs)\n",
    "        hist, edges = np.histogram(xs[filter], bins=buckets, range=(minval, maxval), density=True)\n",
    "        X[index] = np.array(hist)\n",
    "    \n",
    "    return X, minval, maxval, edges\n",
    "\n",
    "def classify(Xtrain, Ytrain, n_neighbors, histnormalise, classifier):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=\"uniform\") if classifier == 'KNN' else tree.DecisionTreeClassifier(max_depth=n_neighbors)\n",
    "    clf2 = clf\n",
    "    scaler = StandardScaler()\n",
    "    if (histnormalise == True):\n",
    "        clf = Pipeline([('scaler', scaler), ('classifier', clf)])\n",
    "    # splits = [(np.arange(0,130,1), np.arange(130,260,1)), (np.arange(130,260,1), np.arange(0,130,1))]\n",
    "    cvs = cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    return cvs.mean(), cvs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter settings\n",
    "types = ['scatter', 'spatstat', 'moran', ]\n",
    "sharpening = ['none', 'direct',]\n",
    "windowsizes = [1, 3, 5, 7]\n",
    "neighbourhoodsize = [3, 5, 7]\n",
    "bins = [5, 10, 20, 40, 80]\n",
    "histnormalise = [True, False]\n",
    "k = [3,5,10,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow settings for iterating\n",
    "types = ['scatter', 'spatstat', 'moran']\n",
    "sharpening = ['none']\n",
    "windowsizes = [1, 3, 5]\n",
    "neighbourhoodSetting = ['grid']\n",
    "bins = [5,10,20,40]\n",
    "histnormalise = [True, False]\n",
    "k = [5,10,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter settings for the report. Might wanna expand down the line?\n",
    "types = ['scatter', 'spatstat', 'moran']\n",
    "classifiers = ['KNN', 'DT']\n",
    "sharpening = ['none']\n",
    "windowsizes = [1, 3, 5, 7]\n",
    "neighbourhoodsize = [3, 5, 7]\n",
    "bins = [5, 10, 20, 40]\n",
    "histnormalise = [True]\n",
    "k = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameter settings for investigating the shittyness\n",
    "# types = ['spatstat']\n",
    "# classifiers = ['KNN']\n",
    "# sharpening = ['none']\n",
    "# windowsizes = [1]\n",
    "# neighbourhoodsize = [3]\n",
    "# bins = [40]\n",
    "# histnormalise = [True]\n",
    "# k = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to tie it all together...\n",
    "def doStuff (outputfile, trainobjects, trainobjectinfo, testobjects, testobjectinfo, classifier):\n",
    "    columns = ['type', 'sharpening', 'windowSize', 'neighbourhood', 'buckets', 'histnormalise', 'k-nearest', 'cv-auc', 'test-auc',]\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    Ytrain = np.array(trainobjectinfo.loc[:,\"Label\"])\n",
    "    Ytest = np.array(testobjectinfo.loc[:,\"Label\"])\n",
    "    for type in types:\n",
    "        for sharpSetting in sharpening:\n",
    "            trainpreprocessed =preprocess(trainobjects, type, sharpSetting)\n",
    "            # testpreprocessed =preprocess(testobjects, type, sharpSetting)\n",
    "            for windowsize in windowsizes:\n",
    "                for nSetting in neighbourhoodsize:\n",
    "                    trainoutliers = calculateoutliers(trainpreprocessed, type, nSetting, windowsize)\n",
    "                    # testoutliers = calculateoutliers(testpreprocessed, type, nSetting, windowsize)\n",
    "                    for histnorm in histnormalise:\n",
    "                        for bincount in bins:\n",
    "                            Xtrain, minval, maxval, edges = encode(trainoutliers, bincount)\n",
    "                            # Xtest, _, _, _ = encode(testoutliers, bincount, minval=minval, maxval=maxval)\n",
    "                            for n_neighbors in k:\n",
    "                                cvscore, score = classify(Xtrain, Ytrain, n_neighbors, histnorm, classifier)\n",
    "                                nextRow =  pd.DataFrame([[type, sharpSetting, windowsize, nSetting, bincount, histnorm, n_neighbors, cvscore, score]], columns=columns)\n",
    "                                results = pd.concat([results, nextRow])\n",
    "                            results.to_csv(outputfile, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching data with objectsplit: 1, upsamplingratio: 1, positive_multiplier: 1, porositythreshold: 0.5\n",
      "fetching data with  0.5\n",
      "Doing stuff for results/A/CW-KNN-0.5-os-1-usr-1-pm-1.csv\n",
      "Doing stuff for results/A/CW-DT-0.5-os-1-usr-1-pm-1.csv\n",
      "fetching data with objectsplit: 1, upsamplingratio: 1, positive_multiplier: 1, porositythreshold: 0.25\n",
      "fetching data with  0.25\n",
      "Doing stuff for results/A/CW-KNN-0.25-os-1-usr-1-pm-1.csv\n",
      "Doing stuff for results/A/CW-DT-0.25-os-1-usr-1-pm-1.csv\n",
      "fetching data with objectsplit: 1, upsamplingratio: 1, positive_multiplier: 1, porositythreshold: 0.1\n",
      "fetching data with  0.1\n",
      "Doing stuff for results/A/CW-KNN-0.1-os-1-usr-1-pm-1.csv\n",
      "Doing stuff for results/A/CW-DT-0.1-os-1-usr-1-pm-1.csv\n"
     ]
    }
   ],
   "source": [
    "from distutils.file_util import write_file\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors, metrics\n",
    "from sklearn import tree\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Read data\n",
    "\n",
    "# Do the things\n",
    "# Check the baseline\n",
    "# for i in [0.1, 0.25, 0.5]:\n",
    "for i in [0.5, 0.25, 0.1]:\n",
    "# for i in [0.25]:\n",
    "    # for (objectsplit, upsamplingratio, positive_multiplier) in [(1,1,1), (2,2,2), (2, 1, 2), (3, 3, 3)]:\n",
    "    # for (objectsplit, upsamplingratio, positive_multiplier) in [(1,1,1), (2,2,2)]:\n",
    "    # for (objectsplit, upsamplingratio, positive_multiplier) in [(3,1,1), (3,1,2)]:\n",
    "    for (objectsplit, upsamplingratio, positive_multiplier) in [(1,1,1)]:\n",
    "    # For each threshold...\n",
    "        trainobjects, trainobjectinfo, testobjects, testobjectinfo = getTrainObjects(porositythreshold=i, objectsplit=objectsplit, upsamplingratio=upsamplingratio, positive_multiplier=positive_multiplier, separate_test=True)\n",
    "        testobjects, testobjectinfo = getTestObjects(porositythreshold=i)\n",
    "        # trainobjectinfo = pd.concat([trainobjectinfo, testobjectinfo])\n",
    "        # trainobjects += testobjects\n",
    "        \n",
    "        # clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "        # Ytrain = np.array(trainobjectinfo.loc[:,\"Label\"])\n",
    "        # Ytest = np.array(testobjectinfo.loc[:,\"Label\"])\n",
    "        # TrainMean = np.array([np.nanmean(object) for object in trainobjects])\n",
    "        # TestMean = np.array([np.nanmean(object) for object in testobjects])\n",
    "        # clf.fit(TrainMean.reshape(-1, 1), Ytrain)\n",
    "        # Ypred = clf.predict_proba(TestMean.reshape(-1, 1))[:,1]\n",
    "        # f = open('results/A/validate-baseline-' + classifier + str(i) + '-os-' + str(objectsplit) + '-usr-' + str(upsamplingratio) + '-pm-' + str(positive_multiplier) + '.txt', \"x\")\n",
    "        # f.write(str(metrics.roc_auc_score(Ytest, Ypred)))\n",
    "        # f.close()\n",
    "        for classifier in classifiers:\n",
    "            filename = 'results/A/CW-' + classifier + '-' + str(i) +  '-os-' + str(objectsplit) + '-usr-' + str(upsamplingratio) + '-pm-' + str(positive_multiplier) + '.csv'\n",
    "            print(\"Doing stuff for \" + filename)\n",
    "            doStuff(filename, trainobjects, trainobjectinfo, testobjects, testobjectinfo, classifier)\n",
    "    # We need plots later, but that's fine because we'll plot the good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = [(np.arange(0,130,1), np.arange(130,260,1)), (np.arange(130,260,1), np.arange(0,130,1))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('thesis-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b582782fd74adb665333939d2eb8c9995c52902e0db289bedb880e3a83e2d4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
