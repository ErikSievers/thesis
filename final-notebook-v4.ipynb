{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the other object as test set ðŸ¤ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus cell for using the different test set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from copy import copy\n",
    "from matplotlib import cm, colors\n",
    "import cv2\n",
    "\n",
    "# TvÃ¥ delar:\n",
    "# 1: fixa segmentstorleken\n",
    "# 2: fixa masker fÃ¶r varje segment\n",
    "# Eller egentligen, skit i det. Applicera masken i fÃ¶rvÃ¤g?\n",
    "\n",
    "\n",
    "def getTestObjects(porositythreshold=0.5): \n",
    "    objectwidth = 100\n",
    "    objectheight = 100\n",
    "    xspacing = 116\n",
    "    yspacing = 300\n",
    "    xstart = 293\n",
    "    ystart = 445\n",
    "    xend = 1730\n",
    "    yend = 1770\n",
    "    powderthickness = 80\n",
    "    endlayer = 225\n",
    "    vsegments = [0, 150, 187, endlayer]\n",
    "\n",
    "    paths = pathlib.Path('./OT data 80 um/int').glob('*.tif')\n",
    "    paths_sorted = [x for x in paths]\n",
    "    paths_sorted.sort()\n",
    "    integrals = np.array([np.array(plt.imread(path)) for path in paths_sorted])\n",
    "\n",
    "    objectinfo = pd.read_csv('Parameters2.csv', names=[\"Object\", \"P\", \"S\", \"H\", \"Porosity\", \"Label\"])\n",
    "    objectCoordinates = [[x, x+objectwidth, y, y+objectheight] for y in reversed(range(\n",
    "        ystart, yend, objectheight + yspacing)) for x in range(xstart, xend, xspacing + objectwidth)]\n",
    "    coorddf = pd.DataFrame(objectCoordinates, columns=['xstart', 'xend', 'ystart', 'yend'])\n",
    "    objectinfo = coorddf.join(objectinfo)\n",
    "\n",
    "    objectinfo.replace('GOOD', 0, inplace=True)\n",
    "    objectinfo.replace('LOF', 1, inplace=True)\n",
    "    objectinfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    objects = np.full((len(objectinfo), endlayer, objectheight, objectwidth), np.nan)\n",
    "\n",
    "    for index, object in objectinfo.iterrows():\n",
    "        objects[index] = integrals[:, object.ystart:object.yend, object.xstart:object.xend]\n",
    "\n",
    "    rtn = np.full(objects.shape, np.nan)\n",
    "    aggregate = np.sum(objects, axis=(0))\n",
    "\n",
    "    emptyRatio = 30\n",
    "    limit = np.percentile(aggregate, emptyRatio)\n",
    "    testmask = aggregate >= limit\n",
    "    for object in objects:\n",
    "        object[~testmask] = np.nan\n",
    "\n",
    "    # Create the good frame\n",
    "    segmentdf = pd.read_csv('Segments2.csv', names=[\"Object\", \"Objectnumber\", \"Segment\", \"P\", \"S\", \"H\", \"Porosity\"])\n",
    "    segmentdf.insert(1, \"VED\", segmentdf.P * 1000/(segmentdf.S * segmentdf.H * powderthickness))\n",
    "    segmentdf.insert(1, \"Label\", np.where(segmentdf.Porosity > porositythreshold, 1, 0))\n",
    "    # segmentdf.drop(segmentdf[(segmentdf.VED > 50)].index, inplace=True)\n",
    "    segmentdf.reset_index(drop=True, inplace=True)\n",
    "    vs = [[vsegments[j], vsegments[j+1]] for i in range(0, len(objects)) for j in reversed(range(0, len(vsegments)-1))]\n",
    "    coorddf = pd.DataFrame(vs, columns=['zstart', 'zend'])\n",
    "    testobjectinfo = coorddf.join(segmentdf)\n",
    "    testobjectinfo.drop(testobjectinfo[(testobjectinfo.Objectnumber == 28) | (testobjectinfo.Objectnumber == 21) ].index, inplace=True)\n",
    "    testobjectinfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    testobjects = [objects[object.Objectnumber-1, object.zstart:object.zend] for index, object in testobjectinfo.iterrows()]\n",
    "\n",
    "    del objects\n",
    "    del objectinfo\n",
    "    del coorddf\n",
    "    del objectCoordinates\n",
    "    del paths_sorted\n",
    "    del integrals\n",
    "    print(\"fetching data with \", porositythreshold)\n",
    "    return testobjects, testobjectinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from copy import copy\n",
    "from matplotlib import cm, colors\n",
    "import cv2\n",
    "\n",
    "def getTrainObjects(objectsplit=1, upsamplingratio=1, positive_multiplier=1, porositythreshold=0.5, separate_test=True):\n",
    "    emptyRatio = 47\n",
    "    objectwidth = 83\n",
    "    objectheight = 122\n",
    "    xspacing = 133\n",
    "    yspacing = 270\n",
    "    xstart = 293\n",
    "    ystart = 268\n",
    "    xend = 1730\n",
    "    yend = 1770\n",
    "    hsegments = [0,26,50,74,98,122]\n",
    "    powderthickness = 80\n",
    "    endlayer = 187\n",
    "\n",
    "    paths = pathlib.Path('./OT data 80 um/int').glob('*.tif')\n",
    "    paths_sorted = [x for x in paths]\n",
    "    paths_sorted.sort()\n",
    "    block = np.array([np.array(plt.imread(path)) for path in paths_sorted])\n",
    "    integrals = block[0:endlayer]\n",
    "\n",
    "    del paths_sorted\n",
    "    objectinfo = pd.read_csv('Parameters.csv', names=[\"Object\", \"P\", \"S\", \"H\", \"Porosity\", \"Label\"])\n",
    "\n",
    "    objectCoordinates = [[x, x+objectwidth, y, y+objectheight] for y in reversed(range(\n",
    "        ystart, yend, objectheight + yspacing)) for x in range(xstart, xend, xspacing + objectwidth)]\n",
    "    coorddf = pd.DataFrame(objectCoordinates, columns=['xstart', 'xend', 'ystart', 'yend'])\n",
    "    objectinfo = coorddf.join(objectinfo)\n",
    "\n",
    "    objects = np.full((len(objectinfo), endlayer, objectheight, objectwidth), np.nan)\n",
    "\n",
    "    for index, object in objectinfo.iterrows():\n",
    "        objects[index] = integrals[:, object.ystart:object.yend, object.xstart:object.xend]\n",
    "\n",
    "    aggregate = np.sum(objects, axis=(0,1))\n",
    "\n",
    "    emptyRatio = 47\n",
    "    limit = np.percentile(aggregate, emptyRatio)\n",
    "    mask = aggregate >= limit\n",
    "    mask = np.repeat([mask], endlayer, 0)\n",
    "\n",
    "    for object in objects:\n",
    "        object[~mask] = np.nan\n",
    "\n",
    "    # Time to construct the \"real\" dataframe\n",
    "\n",
    "    segmentdf = pd.read_csv('Segments.csv', names=[\"Object\", \"Objectnumber\", \"Segment\", \"P\", \"S\", \"H\", \"Porosity\", \"Area\"])\n",
    "    segmentdf.insert(1, \"VED\", segmentdf.P * 1000/(segmentdf.S * segmentdf.H * powderthickness))\n",
    "    segmentdf.insert(1, \"Label\", np.where(segmentdf.Porosity > porositythreshold, 1, 0))\n",
    "    originalframe = segmentdf.copy()\n",
    "    hs = [[hsegments[j], hsegments[j+1]] for i in range(0, len(objects)) for j in range(0, len(hsegments)-1)]\n",
    "    coorddf = pd.DataFrame(hs, columns=['hstart', 'hend'])\n",
    "    segmentdf = coorddf.join(segmentdf)\n",
    "\n",
    "    # Start of object multiplication \n",
    "    layersPerObject = endlayer // objectsplit\n",
    "    testEnd = endlayer if separate_test else endlayer - layersPerObject * (objectsplit // 3)\n",
    "    zs = [segmentdf.copy().assign(zstart=z, zend=z+layersPerObject) for z in range(0, testEnd-layersPerObject+1, layersPerObject//(upsamplingratio * positive_multiplier))]\n",
    "    testzs = [segmentdf.copy().assign(zstart=testEnd, zend=endlayer)]\n",
    "    trainobjectinfo = pd.concat(zs, ignore_index=True)\n",
    "    trainobjectinfo.drop(trainobjectinfo[(trainobjectinfo.Objectnumber == 28) | (trainobjectinfo.Objectnumber == 21) ].index, inplace=True)\n",
    "    trainobjectinfo.reset_index(drop=True, inplace=True)\n",
    "    testobjectinfo = pd.concat(testzs, ignore_index=True)\n",
    "    testobjectinfo.drop(testobjectinfo[(testobjectinfo.Objectnumber == 28) | (testobjectinfo.Objectnumber == 21) ].index, inplace=True)\n",
    "    testobjectinfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Removes extra rows \n",
    "    trainobjectinfo = trainobjectinfo[(trainobjectinfo['Label'] == 1) | (trainobjectinfo['zstart'] % (positive_multiplier) == 0)]\n",
    "\n",
    "    trainobjects = [objects[object.Objectnumber-1, object.zstart:object.zend, object.hstart:object.hend] for index, object in trainobjectinfo.iterrows()]\n",
    "    testobjects = [objects[object.Objectnumber-1, object.zstart:object.zend, object.hstart:object.hend] for index, object in testobjectinfo.iterrows()]\n",
    "\n",
    "    print(\"fetching data with objectsplit: {}, upsamplingratio: {}, positive_multiplier: {}, porositythreshold: {}\".format(objectsplit, upsamplingratio, positive_multiplier, porositythreshold))\n",
    "    return trainobjects, trainobjectinfo, testobjects, testobjectinfo\n",
    "\n",
    "# assert(np.average(np.isfinite(trainobjects)) == 1)\n",
    "# assert(np.average(np.isfinite(testobjects)) == 1)\n",
    "# xs = np.copy(aggregate)\n",
    "# xs[~backgroundmask] = np.nan\n",
    "# plt.imshow(xs)\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, metrics\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import tree\n",
    "import warnings\n",
    "\n",
    "def preprocess(objects, type, sharpening):\n",
    "    rtn = []\n",
    "    # print(rtn.shape)\n",
    "    for index, object in enumerate(objects):\n",
    "        object = np.copy(object)\n",
    "        sharpened = object\n",
    "        if(sharpening != 'none'):\n",
    "            sharpeningKernel = np.array([   [-1, -1,  -1],\n",
    "                                        [-1,  9,  -1],\n",
    "                                        [ -1, -1,  -1]\n",
    "        ]) if sharpening == 'diagonal' else np.array([  [0, -1,  0],\n",
    "                                                        [-1, 5, -1],\n",
    "                                                        [0, -1,  0]])\n",
    "            sharpened = np.array([cv2.filter2D(src=image, ddepth=-1, kernel=sharpeningKernel) for image in object])\n",
    "        # Sharpening is done\n",
    "        if type == 'scatter' or type == 'spatstat':\n",
    "            xs = np.array(sharpened, copy=True, dtype=np.float32)\n",
    "            rtn.append(xs)\n",
    "        elif type == 'moran':\n",
    "            xs = np.array(sharpened, copy=True, dtype=np.float32)\n",
    "            avg = np.nanmean(xs)\n",
    "            stddev = np.nanstd(xs)\n",
    "            xs = (xs - avg) / avg\n",
    "            rtn.append(xs)\n",
    "    return rtn\n",
    "\n",
    "\n",
    "def calculateoutliers(objects, type, neighbourhoodSetting, windowSize):\n",
    "    # c, z, y, x = objects.shape\n",
    "\n",
    "\n",
    "    outlierValues = []\n",
    "    index = 0\n",
    "    # return objects\n",
    "    for object in objects:\n",
    "        object = np.copy(object)\n",
    "        z, y, x = object.shape\n",
    "        # Step 1: calculate neighbourhood\n",
    "        neighbourkernel = np.ones((neighbourhoodSetting, neighbourhoodSetting)) / neighbourhoodSetting**2\n",
    "        flatNeighbourhood = np.array([cv2.filter2D(src=layer, ddepth=-1, kernel=neighbourkernel) for layer in object])\n",
    "        neighbourhoodValues = np.array([\n",
    "            np.sum(flatNeighbourhood[layerIndex-windowSize:layerIndex], axis=0)/windowSize\n",
    "            for layerIndex in range(windowSize, z+1)\n",
    "        ])\n",
    "        # Step 2: calculate outlier\n",
    "        offset = windowSize // 2\n",
    "        endoffset = windowSize - offset - 1\n",
    "\n",
    "        ys = neighbourhoodValues[0:z-windowSize+1]\n",
    "        xs = object[offset:z-endoffset]\n",
    "        filter = np.logical_and(np.isfinite(xs), np.isfinite(ys))\n",
    "\n",
    "        # plt.imshow(xs[0])\n",
    "        # plt.figure()\n",
    "        # plt.imshow(xs[0])\n",
    "        # plt.figure()\n",
    "        # if(index == 58):\n",
    "        #     plt.imshow(xs[0])\n",
    "        #     plt.figure()\n",
    "        #     plt.imshow(ys[0])\n",
    "        #     plt.figure()\n",
    "        #     plt.imshow(filter[0])\n",
    "        #     plt.figure()\n",
    "        #     print(len(np.unique(filter)))\n",
    "        numberOfFilterValues = len(np.unique(filter))\n",
    "        # print(\"filterlength is: \", numberOfFilterValues)\n",
    "        # print(\"index is:\", index)\n",
    "        assert numberOfFilterValues == 2, f\"Expected filter to have two values, got: {numberOfFilterValues}\"\n",
    "        if type == 'spatstat':\n",
    "            outliers = xs - ys\n",
    "            avg = np.mean(outliers[filter])\n",
    "            std = np.std(outliers[filter])\n",
    "            outliers = (outliers - avg) / std\n",
    "            outlierValues.append(outliers)\n",
    "        else:\n",
    "            with warnings.catch_warnings():\n",
    "                line = np.polyfit(xs[filter].flatten(), ys[filter].flatten(), 1)\n",
    "                p = np.poly1d(line)\n",
    "                outlierValues.append(p(xs) - ys)\n",
    "            assert(xs.shape == p(ys).shape)\n",
    "        assert(len(np.unique(outlierValues[index])) > 1)\n",
    "        assert(len(np.unique(np.isfinite(outlierValues[index]))) == 2)\n",
    "        index+=1\n",
    "    return outlierValues\n",
    "\n",
    "def encode(outlierobjects, buckets, minval=0, maxval=0):\n",
    "    numberOfObjects = len(outlierobjects)\n",
    "    X = np.full((numberOfObjects, buckets), np.nan)\n",
    "    raw = np.concatenate([oo.flatten() for oo in outlierobjects])\n",
    "    filter = np.isfinite(raw)\n",
    "    minval = np.min(raw[filter]) if minval == 0 else minval\n",
    "    maxval = np.max(raw[filter]) if maxval == 0 else maxval\n",
    "    for index in range(0, numberOfObjects):\n",
    "        xs = outlierobjects[index]\n",
    "        filter = np.isfinite(xs)\n",
    "        hist, edges = np.histogram(xs[filter], bins=buckets, range=(minval, maxval), density=True)\n",
    "        X[index] = np.array(hist)\n",
    "    \n",
    "    return X, minval, maxval, edges\n",
    "\n",
    "def classify(Xtrain, Ytrain, n_neighbors, histnormalise, classifier):\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=\"uniform\") if classifier == 'KNN' else tree.DecisionTreeClassifier(max_depth=n_neighbors)\n",
    "    clf2 = clf\n",
    "    scaler = StandardScaler()\n",
    "    if (histnormalise == True):\n",
    "        clf = Pipeline([('scaler', scaler), ('classifier', clf)])\n",
    "    # splits = [(np.arange(0,130,1), np.arange(130,260,1)), (np.arange(130,260,1), np.arange(0,130,1))]\n",
    "    cvs = cross_val_score(clf, Xtrain, Ytrain, cv=10, scoring='roc_auc', n_jobs=-1)\n",
    "    return cvs.mean(), cvs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter settings\n",
    "types = ['scatter', 'spatstat', 'moran', ]\n",
    "sharpening = ['none', 'direct',]\n",
    "windowsizes = [1, 3, 5, 7]\n",
    "neighbourhoodsize = [3, 5, 7]\n",
    "bins = [5, 10, 20, 40, 80]\n",
    "histnormalise = [True, False]\n",
    "k = [3,5,10,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow settings for iterating\n",
    "types = ['scatter', 'spatstat', 'moran']\n",
    "sharpening = ['none']\n",
    "windowsizes = [1, 3, 5]\n",
    "neighbourhoodSetting = ['grid']\n",
    "bins = [5,10,20,40]\n",
    "histnormalise = [True, False]\n",
    "k = [5,10,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter settings for the report. Might wanna expand down the line?\n",
    "types = ['scatter', 'spatstat', 'moran']\n",
    "classifiers = ['KNN', 'DT']\n",
    "sharpening = ['none']\n",
    "windowsizes = [1, 3, 5, 7]\n",
    "neighbourhoodsize = [3, 5, 7]\n",
    "bins = [5, 10, 20, 40]\n",
    "histnormalise = [True]\n",
    "k = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter settings for investigating the shittyness\n",
    "types = ['spatstat']\n",
    "classifiers = ['KNN']\n",
    "sharpening = ['none']\n",
    "windowsizes = [1]\n",
    "neighbourhoodsize = [3]\n",
    "bins = [40]\n",
    "histnormalise = [True]\n",
    "k = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to tie it all together...\n",
    "def doStuff (outputfile, trainobjects, trainobjectinfo, testobjects, testobjectinfo, classifier):\n",
    "    columns = ['type', 'sharpening', 'windowSize', 'neighbourhood', 'buckets', 'histnormalise', 'k-nearest', 'cv-auc', 'test-auc',]\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    Ytrain = np.array(trainobjectinfo.loc[:,\"Label\"])\n",
    "    Ytest = np.array(testobjectinfo.loc[:,\"Label\"])\n",
    "    for type in types:\n",
    "        for sharpSetting in sharpening:\n",
    "            trainpreprocessed =preprocess(trainobjects, type, sharpSetting)\n",
    "            testpreprocessed =preprocess(testobjects, type, sharpSetting)\n",
    "            for windowsize in windowsizes:\n",
    "                for nSetting in neighbourhoodsize:\n",
    "                    trainoutliers = calculateoutliers(trainpreprocessed, type, nSetting, windowsize)\n",
    "                    testoutliers = calculateoutliers(testpreprocessed, type, nSetting, windowsize)\n",
    "                    for histnorm in histnormalise:\n",
    "                        for bincount in bins:\n",
    "                            Xtrain, minval, maxval, edges = encode(trainoutliers, bincount)\n",
    "                            Xtest, _, _, _ = encode(testoutliers, bincount, minval=minval, maxval=maxval)\n",
    "                            for n_neighbors in k:\n",
    "                                cvscore, score = classify(Xtrain, Ytrain, Xtest, Ytest, n_neighbors, histnorm, classifier)\n",
    "                                nextRow =  pd.DataFrame([[type, sharpSetting, windowsize, nSetting, bincount, histnorm, n_neighbors, cvscore, score]], columns=columns)\n",
    "                                results = pd.concat([results, nextRow])\n",
    "                            results.to_csv(outputfile, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching data with objectsplit: 1, upsamplingratio: 1, positive_multiplier: 1, porositythreshold: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/k1mwdb1x1sb5ly3k528g3qcr0000gn/T/ipykernel_70501/3389386378.py:26: RuntimeWarning: Mean of empty slice\n",
      "  TestMean = np.array([np.nanmean(object) for object in testobjects])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/erik/Git/thesis/final-notebook-v4.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Git/thesis/final-notebook-v4.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m TestMean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mnanmean(\u001b[39mobject\u001b[39m) \u001b[39mfor\u001b[39;00m \u001b[39mobject\u001b[39m \u001b[39min\u001b[39;00m testobjects])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Git/thesis/final-notebook-v4.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(TrainMean\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), Ytrain)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erik/Git/thesis/final-notebook-v4.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m Ypred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mpredict_proba(TestMean\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Git/thesis/final-notebook-v4.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults/A/validate-baseline-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m classifier \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-os-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(objectsplit) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-usr-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(upsamplingratio) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-pm-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(positive_multiplier) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erik/Git/thesis/final-notebook-v4.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(metrics\u001b[39m.\u001b[39mroc_auc_score(Ytest, Ypred)))\n",
      "File \u001b[0;32m~/Git/thesis/thesis-env/lib/python3.9/site-packages/sklearn/tree/_classes.py:1002\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39m\"\"\"Predict class probabilities of the input samples X.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \n\u001b[1;32m    980\u001b[0m \u001b[39mThe predicted class probability is the fraction of samples of the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[39m    classes corresponds to that in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1002\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X, check_input)\n\u001b[1;32m   1003\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m   1005\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Git/thesis/thesis-env/lib/python3.9/site-packages/sklearn/tree/_classes.py:471\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[0;32m--> 471\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    472\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    473\u001b[0m         X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc\n\u001b[1;32m    474\u001b[0m     ):\n\u001b[1;32m    475\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Git/thesis/thesis-env/lib/python3.9/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/Git/thesis/thesis-env/lib/python3.9/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[1;32m    900\u001b[0m             array,\n\u001b[1;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    904\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Git/thesis/thesis-env/lib/python3.9/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from distutils.file_util import write_file\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors, metrics\n",
    "from sklearn import tree\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Read data\n",
    "\n",
    "# Do the things\n",
    "# Check the baseline\n",
    "# for i in [0.1, 0.25, 0.5]:\n",
    "for i in [0.1, 0.25, 0.5]:\n",
    "# for i in [0.25]:\n",
    "    # for (objectsplit, upsamplingratio, positive_multiplier) in [(1,1,1), (2,2,2), (2, 1, 2), (3, 3, 3)]:\n",
    "    # for (objectsplit, upsamplingratio, positive_multiplier) in [(1,1,1), (2,2,2)]:\n",
    "    # for (objectsplit, upsamplingratio, positive_multiplier) in [(3,1,1), (3,1,2)]:\n",
    "    for (objectsplit, upsamplingratio, positive_multiplier) in [(1,1,1)]:\n",
    "    # For each threshold...\n",
    "        trainobjects, trainobjectinfo, testobjects, testobjectinfo = getTrainObjects(porositythreshold=i, objectsplit=objectsplit, upsamplingratio=upsamplingratio, positive_multiplier=positive_multiplier, separate_test=True)\n",
    "        # testobjects, testobjectinfo = getTestObjects(porositythreshold=i)\n",
    "        \n",
    "        # clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "        # Ytrain = np.array(trainobjectinfo.loc[:,\"Label\"])\n",
    "        # Ytest = np.array(testobjectinfo.loc[:,\"Label\"])\n",
    "        # TrainMean = np.array([np.nanmean(object) for object in trainobjects])\n",
    "        # TestMean = np.array([np.nanmean(object) for object in testobjects])\n",
    "        # clf.fit(TrainMean.reshape(-1, 1), Ytrain)\n",
    "        # Ypred = clf.predict_proba(TestMean.reshape(-1, 1))[:,1]\n",
    "        # f = open('results/A/validate-baseline-' + classifier + str(i) + '-os-' + str(objectsplit) + '-usr-' + str(upsamplingratio) + '-pm-' + str(positive_multiplier) + '.txt', \"x\")\n",
    "        # f.write(str(metrics.roc_auc_score(Ytest, Ypred)))\n",
    "        # f.close()\n",
    "        for classifier in classifiers:\n",
    "            filename = 'results/A/CW-' + classifier + '-' + str(i) +  '-os-' + str(objectsplit) + '-usr-' + str(upsamplingratio) + '-pm-' + str(positive_multiplier) + '.csv'\n",
    "            print(\"Doing stuff for \" + filename)\n",
    "            doStuff(filename, trainobjects, trainobjectinfo, testobjects, testobjectinfo, classifier)\n",
    "    # We need plots later, but that's fine because we'll plot the good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = [(np.arange(0,130,1), np.arange(130,260,1)), (np.arange(130,260,1), np.arange(0,130,1))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('thesis-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b582782fd74adb665333939d2eb8c9995c52902e0db289bedb880e3a83e2d4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
